{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "957dc243-63d7-4568-aaaf-58c8e2cb7409",
   "metadata": {},
   "source": [
    "# Vertex AI MLOps Book - Chapter 15 - Recommender System - Movie Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a614a5aa-ed71-46b7-acd2-7cde957f90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Supporting code for: Vertex AI ML Ops Code - Chapter 15 - Recommender Systems\n",
    "#Reference: \n",
    "#https://keras.io/examples/structured_data/collaborative_filtering_movielens/\n",
    "\n",
    " # https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/model_registry/get_started_with_model_registry.ipynb\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at: https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79a4052b-0a6d-43fd-9b14-6181b267536e",
   "metadata": {},
   "source": [
    "### Important Note: \n",
    "This notebook might deploy and consume cloud resources in your Google Cloud Project(s) leading to you getting charged/billed for those resources. It's your respondibility to verify the impact of this code before you run it and to monitor and delete any resources to avoid ongoing cloud charges. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06147089",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "##### In this notebook we will create a movie recommendation system using Keras. We will use the MovieLens dataset to train our model. \n",
    "\n",
    "##### We will use the following steps to create our recommendation system:\n",
    "* Download the MovieLens dataset\n",
    "* Train a Collaborative Filtering model to recommend Movies\n",
    "* Upload the Model to Vertex AI Model Registry\n",
    "* Deploy the Model to Vertex AI Endpoint for serving\n",
    "\n",
    "\n",
    "Data Source: https://grouplens.org/datasets/movielens/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fedd1c",
   "metadata": {},
   "source": [
    "## Set up model resources\n",
    " Configure the GCP Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "422589d7-1d02-468b-bd84-d72ae48aa39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell we are \n",
    "PROJECT_ID = \"jsb-alto\"  # @param {type:\"string\"} \n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16f3a510",
   "metadata": {},
   "source": [
    "### Create a GCS Bucket to store model artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5526faf-b77e-4056-8fd2-a50607ac1318",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the bucket name\n",
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the region\n",
    "REGION = \"us-central1\" \n",
    "\n",
    "# Create the bucket\n",
    "! gsutil mb -l {REGION} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313fbc65",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92aa65-4607-442f-abd6-288ce975ef5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from zipfile import ZipFile\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "#Set HW config\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "579c746b",
   "metadata": {},
   "source": [
    "### Download and load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32b9154c-cb47-4a2a-b020-6f2ec32e354c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the actual data from http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    "movielens_data_url = (\n",
    "    \"http://files.grouplens.org/datasets/movielens/ml-latest-small.zip\"\n",
    ")\n",
    "movielens_zip_file = keras.utils.get_file(\n",
    "    \"ml-latest-small.zip\", movielens_data_url, extract=False\n",
    ")\n",
    "\n",
    "movie_datasets_path = Path(movielens_zip_file).parents[0]\n",
    "movielens_dir = movie_datasets_path / \"ml-latest-small\"\n",
    "\n",
    "if not movielens_dir.exists():\n",
    "    with ZipFile(movielens_zip_file, \"r\") as zip:\n",
    "        # Extract files\n",
    "        print(\"Extracting all the files now...\")\n",
    "        zip.extractall(path=movie_datasets_path)\n",
    "        print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ccd216-df3d-4fd7-839e-27b28aed9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Movie Ratings file\n",
    "ratings_file = movielens_dir / \"ratings.csv\"\n",
    "df = pd.read_csv(ratings_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c8fb30-9536-4965-9d2f-21e326c55a24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2e782dda-46fa-4f8d-9596-04e0dd07c227",
   "metadata": {},
   "source": [
    "\n",
    "### Pre process the data to encode users and movies as integer indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b88a47d-96e4-4053-901c-b9bc145c3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the unique user IDs from the 'userId' column and convert them to a list\n",
    "user_ids = df[\"userId\"].unique().tolist()\n",
    "\n",
    "\n",
    "# Create a dictionary that maps each user ID to a unique integer (encoded form)\n",
    "user2user_encoded = {x: i for i, x in enumerate(user_ids)}\n",
    "\n",
    "# Create a dictionary that maps each unique integer back to its original user ID\n",
    "userencoded2user = {i: x for i, x in enumerate(user_ids)}\n",
    "\n",
    "# Extract the unique movie IDs from the 'movieId' column and convert them to a list\n",
    "movie_ids = df[\"movieId\"].unique().tolist()\n",
    "\n",
    "# Create a dictionary that maps each movie ID to a unique integer (encoded form)\n",
    "movie2movie_encoded = {x: i for i, x in enumerate(movie_ids)}\n",
    "\n",
    "# Create a dictionary that maps each unique integer back to its original movie ID\n",
    "movie_encoded2movie = {i: x for i, x in enumerate(movie_ids)}\n",
    "\n",
    "# Map the original user IDs in the 'userId' column to their encoded forms and store in a new column 'user'\n",
    "df[\"user\"] = df[\"userId\"].map(user2user_encoded)\n",
    "\n",
    "# Map the original movie IDs in the 'movieId' column to their encoded forms and store in a new column 'movie'\n",
    "df[\"movie\"] = df[\"movieId\"].map(movie2movie_encoded)\n",
    "\n",
    "# Store the number of users and movies in variables\n",
    "num_users = len(user2user_encoded)\n",
    "num_movies = len(movie_encoded2movie)\n",
    "\n",
    "\n",
    "df[\"rating\"] = df[\"rating\"].values.astype(np.float32)\n",
    "# min and max ratings will be used to normalize the ratings later\n",
    "min_rating = min(df[\"rating\"])\n",
    "max_rating = max(df[\"rating\"])\n",
    "\n",
    "print(\n",
    "    \"Number of users: {}, Number of Movies: {}, Min rating: {}, Max rating: {}\".format(\n",
    "        num_users, num_movies, min_rating, max_rating\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9998cc30-7904-4174-940a-3be0acfa647c",
   "metadata": {},
   "source": [
    "### Split data into train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa832b0-9ab3-49d9-be92-470a996bee81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training and validation data\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "x = df[[\"user\", \"movie\"]].values\n",
    "\n",
    "# Normalize the rating values\n",
    "y = df[\"rating\"].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values\n",
    "\n",
    "# Split 90% of data for training and 10% for validation\n",
    "train_indices = int(0.9 * df.shape[0])\n",
    "x_train, x_val, y_train, y_val = (\n",
    "    x[:train_indices],\n",
    "    x[train_indices:],\n",
    "    y[:train_indices],\n",
    "    y[train_indices:],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c3c370",
   "metadata": {},
   "source": [
    "## Create the model\n",
    "\n",
    "We embed both users and movies in to 50-dimensional vectors.\n",
    "The model computes a match score between user and movie embeddings via a dot product,\n",
    "and adds a per-movie and per-user bias. The match score is scaled to the `[0, 1]`\n",
    "interval via a sigmoid (since our ratings are normalized to this range).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25993d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size for embeddings\n",
    "EMBEDDING_SIZE = 50\n",
    "\n",
    "\n",
    "# Define the RecommendationModel class, which is a subclass of the keras.Model\n",
    "class RecommendationModel(keras.Model):\n",
    "    def __init__(self, num_users, num_movies, embedding_size, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_users = num_users\n",
    "        self.num_movies = num_movies\n",
    "        self.embedding_size = embedding_size\n",
    "        \n",
    "        # User embeddings layer: Represents each user as a vector in the embedding space\n",
    "        self.user_embedding = layers.Embedding(\n",
    "            num_users,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.user_bias = layers.Embedding(num_users, 1)\n",
    "\n",
    "        # Movie embeddings layer: Represents each movie as a vector in the embedding space\n",
    "        self.movie_embedding = layers.Embedding(\n",
    "            num_movies,\n",
    "            embedding_size,\n",
    "            embeddings_initializer=\"he_normal\",\n",
    "            embeddings_regularizer=keras.regularizers.l2(1e-6),\n",
    "        )\n",
    "        self.movie_bias = layers.Embedding(num_movies, 1)\n",
    "        \n",
    "     # Forward pass: Given user and movie IDs, predict the rating\n",
    "    def call(self, inputs):\n",
    "        user_vector = self.user_embedding(inputs[:, 0])\n",
    "        user_bias = self.user_bias(inputs[:, 0])\n",
    "        movie_vector = self.movie_embedding(inputs[:, 1])\n",
    "        movie_bias = self.movie_bias(inputs[:, 1])\n",
    "        dot_user_movie = tf.tensordot(user_vector, movie_vector, 2)\n",
    "        # Add all the components (including bias)\n",
    "        x = dot_user_movie + user_bias + movie_bias\n",
    "        # The sigmoid activation forces the rating to between 0 and 1\n",
    "        return tf.nn.sigmoid(x)\n",
    "\n",
    "\n",
    "# # Instantiate the Recommender model with the defined number of users, movies, and embedding size \n",
    "model = RecommendationModel(num_users, num_movies, EMBEDDING_SIZE)\n",
    "\n",
    "# Compile the Recommender model\n",
    "model.compile(\n",
    "    loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a303bba-ad4b-4bd2-80a3-0fbb84629a70",
   "metadata": {},
   "source": [
    "### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e57b18-1388-4ad5-9cfd-833e728ca227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    batch_size=64,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=(x_val, y_val),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c61dd9b-445c-4267-b974-9a575656e057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cef5abe-09ef-41b6-8326-e5325591333e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f75554a6-c5ab-4ff4-82b8-df27916fe0c8",
   "metadata": {},
   "source": [
    "### Plot the training and validation loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f59e3bc2-d48f-49b8-9b2b-bbf4c6e66b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history[\"loss\"])\n",
    "plt.plot(history.history[\"val_loss\"])\n",
    "plt.title(\"model loss\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.legend([\"train\", \"test\"], loc=\"upper left\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0c53e4-a8a6-43f0-b1f9-047e6b75c29e",
   "metadata": {},
   "source": [
    "## Test model predictions locally "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8077f6-f04f-4481-8a8d-3e3d7f849353",
   "metadata": {},
   "source": [
    "Create an array with unwatched movies for a random user to use as sample data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47237e64-94ef-4838-b2e3-a374bc22dd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Load the Movie Metadata\n",
    "movie_df = pd.read_csv(movielens_dir / \"movies.csv\")\n",
    "\n",
    "# Let us get a user and see the top recommendations.\n",
    "user_id = df.userId.sample(1).iloc[0]\n",
    "movies_watched_by_user = df[df.userId == user_id]\n",
    "movies_not_watched = movie_df[\n",
    "    ~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)\n",
    "][\"movieId\"]\n",
    "\n",
    "movies_not_watched = list(\n",
    "    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n",
    ")\n",
    "\n",
    "movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n",
    "user_encoder = user2user_encoded.get(user_id)\n",
    "user_prediction_array = np.hstack(\n",
    "    ([[user_encoder]] * len(movies_not_watched), movies_not_watched)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd60f00-7c8e-4bd2-9b6f-ce13767a65cf",
   "metadata": {},
   "source": [
    "Predict the ratings for unwatched movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e991bc2a-88b3-442e-8649-875045157e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get predicted ratings for the unwatched movies and the selected user\n",
    "ratings = model.predict(user_prediction_array).flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64697b77-2d13-4d58-9e2d-8d63e450ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort and pick top 10 ratings\n",
    "movie_indices_top10 = ratings.argsort()[-10:][::-1]\n",
    "\n",
    "movie_recommendations_ids = [\n",
    "    movie_encoded2movie.get(movies_not_watched[x][0]) for x in movie_indices_top10\n",
    "]\n",
    "\n",
    "print(\"----\" * 10)\n",
    "print(\"Top movies recommendations for user id: {}\".format(user_id))\n",
    "print(\"----\" * 10)\n",
    "recommended_movies = movie_df[movie_df[\"movieId\"].isin(movie_recommendations_ids)]\n",
    "for row in recommended_movies.itertuples():\n",
    "    print(row.title, \":\", row.genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56991191-4f3b-48a6-96cb-3577412e4502",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "103b28b2-f56a-437b-84c5-de75e9e67ccb",
   "metadata": {},
   "source": [
    "### Save the model to GCS bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ac4132-b683-430e-8ae0-d9654002ba2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model in GCS bucket so that we can import it into Vertex AI Model Registry\n",
    "MODEL_DIR = BUCKET_URI + \"/model/\"\n",
    "model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04164bae",
   "metadata": {},
   "source": [
    "## Deploy the model to Vertex AI for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45a4092-5dff-4086-a637-a044ff7b740b",
   "metadata": {},
   "source": [
    "### Upload the model to Vertex AI Model Registry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024e6e8f-ff80-4232-aff7-9aed627f6f4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## aip.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)\n",
    "\n",
    "DEPLOY_GPU, DEPLOY_NGPU = (None, None)\n",
    "TF = \"2.12\".replace(\".\", \"-\")\n",
    "\n",
    "if DEPLOY_GPU:\n",
    "    DEPLOY_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "else:\n",
    "    DEPLOY_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "\n",
    "\n",
    "DEPLOY_IMAGE = \"{}-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(\n",
    "    REGION.split(\"-\")[0], DEPLOY_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2da58f-2e41-44df-8c44-e2c4e3508ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = aip.Model.upload(\n",
    "    display_name=\"recommender_model_chp15\",\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    is_default_version=True,\n",
    "    version_aliases=[\"v1\"],\n",
    "    version_description=\"This is the first version of the model\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9e553f-3d61-4c4a-87c3-a23c9311a129",
   "metadata": {},
   "outputs": [],
   "source": [
    "versions = model.versioning_registry.list_versions()\n",
    "for version in versions:\n",
    "    print(version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec34a38-d860-48f6-8d80-7f346da47c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## List Model versions\n",
    "models = aip.Model.list(filter=\"display_name=recommender_model_chp15\")\n",
    "print(\"Number of models:\", len(models))\n",
    "print(\"Version ID:\", models[0].version_id)\n",
    "\n",
    "model = models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c1e4b1-118e-4711-944b-01a51b94e1cb",
   "metadata": {
    "id": "628de0914ba1"
   },
   "source": [
    "## Creating an `Endpoint` resource\n",
    "\n",
    "- `display_name`: A human readable name for the `Endpoint` resource.\n",
    "- `project`: Your project ID.\n",
    "- `location`: Your region.\n",
    "- `labels`: (optional) User defined metadata for the `Endpoint` in the form of key/value pairs.\n",
    "\n",
    "This method returns an `Endpoint` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200d83b7-d567-496c-9848-c7a240652a1b",
   "metadata": {
    "id": "0ea443f9593b"
   },
   "outputs": [],
   "source": [
    "endpoint = aip.Endpoint.create(\n",
    "    display_name=\"recommender_model_chp15\",\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    \n",
    ")\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b56d72-dcf0-4727-b30c-40a0e43aa038",
   "metadata": {
    "id": "ca3fa3f6a894"
   },
   "source": [
    "### Deploying Models to Endpoints in Vertex AI\n",
    "Vertex AI allows for the deployment of one or more Model resource instances to a single endpoint. Notably, each deployed Vertex AI Model has its dedicated deployment container for the serving binary.\n",
    "\n",
    "\n",
    "#### Deploying to an Endpoint\n",
    "Below, we'll demonstrate how to deploy the default version of a Vertex AI Model resource to a Vertex AI Endpoint. The deployment container image for this model is already specified. However, when deploying, you need to provide additional configurations such as:\n",
    "\n",
    "* Machine Type: Defines the compute capacity for serving the model.\n",
    "* GPU Configuration: Optional settings defining type and number of GPUs.\n",
    "* Scaling: Choices include static, manual, or auto-scaling for VM instances.\n",
    "\n",
    "For this deployment example, we're keeping it straightforward by specifying:\n",
    "\n",
    "* model: Refers to the Model resource.\n",
    "* deployed_model_display_name: A user-friendly name for the deployed model.\n",
    "* machine_type: Specifies the type of VM for each instance.\n",
    "\n",
    "\n",
    "Note: Due to the underlying provisioning processes, the deployment might take a few minutes. Please be patient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Deployment:\", DEPLOY_IMAGE, DEPLOY_GPU, DEPLOY_NGPU)\n",
    "\n",
    "DEPLOY_COMPUTE = \"n1-standard-4\"\n",
    "print(\"Train machine type\", DEPLOY_COMPUTE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf3761-b59b-4858-bc92-5de5acbc517f",
   "metadata": {
    "id": "4e93b034a72f"
   },
   "outputs": [],
   "source": [
    "#Deploy the model to the Vertex AI endpoint\n",
    "response = endpoint.deploy(\n",
    "    model=model,\n",
    "    deployed_model_display_name=\"example_\",\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    ")\n",
    "\n",
    "print(endpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09537e5c-6568-4f9b-9fc6-5895cac39853",
   "metadata": {
    "id": "f1ae5a228adb"
   },
   "source": [
    "#### Get information on the deployed model\n",
    "\n",
    "You can get the deployment settings of the deployed model from the `Endpoint` resource configuration data `gca_resource.deployed_models`. In this example, only one model is deployed -- hence the reference to the subscript `[0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ed348f-731d-4d0d-a2e4-88c558d4f145",
   "metadata": {
    "id": "5864deb1fd90"
   },
   "outputs": [],
   "source": [
    "endpoint.gca_resource.deployed_models[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1ba61a-6856-4fbc-9c50-3838645cfbfe",
   "metadata": {},
   "source": [
    "## Run inference on the Vertex AI model Endpoint "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6ce027-e9a5-4454-a8e3-e31501b4682d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the Inference function\n",
    "\n",
    "from typing import Dict, List, Union\n",
    "from google.cloud import aiplatform\n",
    "from google.protobuf import json_format\n",
    "from google.protobuf.struct_pb2 import Value\n",
    "\n",
    "\n",
    "def predict_custom_trained_model_sample(\n",
    "    project: str,\n",
    "    endpoint_id: str,\n",
    "    instances: Union[Dict, List[Dict]],\n",
    "    location: str = \"us-central1\",\n",
    "    api_endpoint: str = \"us-central1-aiplatform.googleapis.com\",\n",
    "):\n",
    "    \"\"\"\n",
    "    `instances` can be either single instance of type dict or a list\n",
    "    of instances.\n",
    "    \"\"\"\n",
    "    # The AI Platform services require regional API endpoints.\n",
    "    client_options = {\"api_endpoint\": api_endpoint}\n",
    "    # Initialize client that will be used to create and send requests.\n",
    "    # This client only needs to be created once, and can be reused for multiple requests.\n",
    "    client = aiplatform.gapic.PredictionServiceClient(client_options=client_options)\n",
    "    # The format of each instance should conform to the deployed model's prediction input schema.\n",
    "    #instances = instances if isinstance(instances, list) else [instances]\n",
    "    instances = [\n",
    "        json_format.ParseDict(instance_dict, Value()) for instance_dict in instances\n",
    "    ]\n",
    "    parameters_dict = {}\n",
    "    parameters = json_format.ParseDict(parameters_dict, Value())\n",
    "    endpoint = client.endpoint_path(\n",
    "        project=project, location=location, endpoint=endpoint_id\n",
    "    )\n",
    "    response = client.predict(\n",
    "        endpoint=endpoint, instances=instances, parameters=parameters\n",
    "    )\n",
    "    print(\"response\")\n",
    "    print(\" deployed_model_id:\", response.deployed_model_id)\n",
    "    # The predictions are a google.protobuf.Value representation of the model's predictions.\n",
    "    predictions = response.predictions\n",
    "    #for prediction in predictions:\n",
    "    #    print(\" prediction:\", dict(prediction))\n",
    "    return(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b81258-0c04-4b32-9d09-d8622002ea48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cdcbe72-1ad7-4cb0-bd88-933fbd88dbf3",
   "metadata": {},
   "source": [
    "# Create sample inference dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ea30fc-5129-442b-b3c5-52dd670cb05c",
   "metadata": {},
   "source": [
    "### Get a user and see the top recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b5f0f-d563-46b2-8bde-88df49cea531",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us get a user and see the top recommendations. Pick Random User\n",
    "user_id = df.userId.sample(1).iloc[0]\n",
    "#user_id = 41\n",
    "\n",
    "#Add filter for the category for which you need recommendations\n",
    "genre_filter = \"Drama\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e3ced4-c7cb-4d03-a4bc-1c66885cb328",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5db49f-0378-499c-84f6-904f91bdc625",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Test Dataset for a User and the selected Genre\n",
    "movie_df = pd.read_csv(movielens_dir / \"movies.csv\")\n",
    "\n",
    "movies_watched_by_user = df[df.userId == user_id]\n",
    "#movies_not_watched = movie_df[~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values) ][\"movieId\"]\n",
    "\n",
    "#Create Dataframe with Movies not watched by the User\n",
    "movies_not_watched_df = movie_df[\n",
    "    #(~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)) & (movie_df[\"genres\"] == genre_filter)\n",
    "    (~movie_df[\"movieId\"].isin(movies_watched_by_user.movieId.values)) & (movie_df[\"genres\"].str.contains(genre_filter))\n",
    "][[\"movieId\",\"title\",\"genres\"]]\n",
    "\n",
    "\n",
    "#movies_not_watched_df.reset_index(inplace = True,drop=True)\n",
    "movies_not_watched = movies_not_watched_df[\"movieId\"]\n",
    "#movies_not_watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4677d121-bc00-4a0d-9d11-833d279b0fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "movies_not_watched = list(\n",
    "    set(movies_not_watched).intersection(set(movie2movie_encoded.keys()))\n",
    ")\n",
    "movies_not_watched = [[movie2movie_encoded.get(x)] for x in movies_not_watched]\n",
    "#movies_not_watched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cb2e98-5d3e-4ebd-a95f-a828f60d5810",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_encoder = user2user_encoded.get(user_id)\n",
    "user_prediction_array = np.hstack(([[user_encoder]] * len(movies_not_watched), movies_not_watched) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c49105-9440-4375-b59f-1dabc6ada10d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26bf22bb-db1c-49a4-b160-09bb4cb8e0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create data instances that would be sent to the API for inference\n",
    "instances = user_prediction_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d277ecdf-150e-4cc3-94d4-2bfe6a070879",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predicted ratings for the unwatched movies and the selected user\n",
    "predictions = predict_custom_trained_model_sample(\n",
    "    project=endpoint.project,\n",
    "    endpoint_id=endpoint.name,\n",
    "    location=endpoint.location,\n",
    "    instances = instances\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2275a1a-1258-44f6-b82e-784c113e8988",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame from the predictions list/array\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "\n",
    "# Rename the column in the predictions DataFrame to 'rating'\n",
    "predictions_df.columns = ['rating']\n",
    "\n",
    "# Create a DataFrame from the instances list/array\n",
    "instances_df = pd.DataFrame(instances)\n",
    "\n",
    "# Rename the columns in the instances DataFrame to 'userId' and 'movieId' respectively\n",
    "instances_df.columns = ['userId','movieId']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5947c9b1-0cd1-4123-af66-d38d93a3d113",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the instances and predictions DataFrames \n",
    "combined_results = instances_df.join(predictions_df)\n",
    "#combined_results\n",
    "\n",
    "# Sort the results by the rating column in descending order\n",
    "combined_results_sorted = combined_results.sort_values('rating',ascending=False)\n",
    "#combined_results_sorted\n",
    "\n",
    "# Filter the results to show only the top 20 results\n",
    "combined_results_sorted_top = combined_results_sorted.head(20)[\"movieId\"].values\n",
    "#combined_results_sorted_top"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03b3753-c80f-42b0-b218-9267aa81dbe1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0ec9db-623a-4745-ba22-c6d5979aa8bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d561c2-451d-4107-9989-698196e1e41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map the encoded Movie IDs to the actual Movie IDs\n",
    "recommended_movie_ids = [\n",
    "    movie_encoded2movie.get(x) for x in combined_results_sorted_top\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe4bda-430b-4135-8e65-553432ded8b7",
   "metadata": {},
   "source": [
    "## Recommended movie ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f5e9cf-4c34-43dd-8f25-81d59c8ff0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"----\" * 10)\n",
    "print(\"Top 20 recommended movies recommendations for User:\",user_id,\" and Genre\",genre_filter)\n",
    "print(\"Genre:\",genre_filter)\n",
    "print(\"----\" * 10)\n",
    "recommended_movies = movie_df[movie_df[\"movieId\"].isin(recommended_movie_ids)]\n",
    "for row in recommended_movies.itertuples():\n",
    "    print(row.title, \":\", row.genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191aea9c-a81a-4d91-9f46-964d6dbaf7f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c102330-0d11-4293-8d02-909a5599d6ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-12.m110",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-12:m110"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
