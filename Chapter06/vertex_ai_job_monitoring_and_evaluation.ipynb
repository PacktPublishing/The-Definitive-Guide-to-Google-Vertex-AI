{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80af36d-be92-44bb-b84f-e7de7d7a038c",
   "metadata": {},
   "source": [
    "# Convert Black-n-White image to Color Images - Vertex AI Job Monitoring and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3568b64-9299-4eb8-a99d-3b7c6ddda61b",
   "metadata": {},
   "source": [
    "## Install useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23baac0a-f082-43f3-a564-20d37bb6694f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.21.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.24.0-py2.py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (2.7.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.8.0-py2.py3-none-any.whl (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (9.4.0)\n",
      "Collecting pillow\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.22.3-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (3.4.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.8.1)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.22.2)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.10.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.28.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.3.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.58.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.51.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.9.1-py2.py3-none-any.whl (276 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.9.0-py2.py3-none-any.whl (276 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.8.0-py2.py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.7.0-py2.py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
      "Installing collected packages: protobuf, pillow, packaging, google-cloud-storage, google-cloud-resource-manager, google-cloud-aiplatform\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.4.0\n",
      "    Uninstalling Pillow-9.4.0:\n",
      "      Successfully uninstalled Pillow-9.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.7.0\n",
      "    Uninstalling google-cloud-storage-2.7.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.7.0\n",
      "  Attempting uninstall: google-cloud-resource-manager\n",
      "    Found existing installation: google-cloud-resource-manager 1.8.1\n",
      "    Uninstalling google-cloud-resource-manager-1.8.1:\n",
      "      Successfully uninstalled google-cloud-resource-manager-1.8.1\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.21.0\n",
      "    Uninstalling google-cloud-aiplatform-1.21.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.4 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorboard 2.8.0 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 0.8.0 which is incompatible.\n",
      "google-cloud-vision 3.3.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-videointelligence 1.16.3 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-spanner 3.27.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-pubsub 2.14.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-language 1.3.2 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-dlp 3.11.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.13.2 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.22.3 which is incompatible.\n",
      "apache-beam 2.44.0 requires httplib2<0.21.0,>=0.8, but you have httplib2 0.21.0 which is incompatible.\n",
      "apache-beam 2.44.0 requires protobuf<4,>3.12.2, but you have protobuf 4.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.24.0 google-cloud-resource-manager-1.6.3 google-cloud-storage-2.8.0 packaging-21.3 pillow-9.5.0 protobuf-4.22.3\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd52bf-29ab-4851-9b21-ff0ad7814b84",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e93550-5b0d-4558-9cd8-6c5fb7465f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from google.cloud import aiplatform\n",
    "from datetime import datetime\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61787ad8-d88a-4129-b521-7456074c6a49",
   "metadata": {},
   "source": [
    "## Setup project configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c80379c-99e9-495e-89bf-d0fea02db474",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='417812395597'\n",
    "REGION='us-west2'\n",
    "BUCKET_URI='gs://my-training-artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657b6b8-17b0-4e7a-a454-d31fe3018161",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI (SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0108c61b-7a06-48c8-bffd-7da85f2bbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa928772-f392-4917-b0c0-1ab5c6bff483",
   "metadata": {},
   "source": [
    "## Setup pre-built contianer for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10544f7-f248-4221-bcc0-d324aa1e6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"tf-cpu.2-9\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-9\"\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca7270-4c5b-4900-9dee-630ce55942c0",
   "metadata": {},
   "source": [
    "## Define command-line arguments for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e7bcda-457a-4802-a3d9-325d3a6c9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"vertex_custom_training\"\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
    "\n",
    "TRAIN_STRATEGY = \"single\"\n",
    "EPOCHS = 20\n",
    "STEPS = 100\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--steps=\" + str(STEPS),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0dbed-e186-4361-bcb1-67feab3ed238",
   "metadata": {},
   "source": [
    "## Writing down entire training script into a \"task.py\" file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2748a-4e7c-40d2-903d-63f7a8121e2e",
   "metadata": {},
   "source": [
    "### This task.py will run inside the contianer that we have defined above (could be a custom contianer with all dependencies)\n",
    "#### task.py should have the entire training flow including - \n",
    "- Load and prepared the training data\n",
    "- define model architecure\n",
    "- train model\n",
    "- save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb42302-2dc4-4f31-9489-3dcddde878ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing task2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task2.py\n",
    "# Single, Mirror and Multi-Machine Distributed Training for CIFAR-10\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "# parse required arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.001, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=35, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('Python Version = {}'.format(sys.version))\n",
    "print('TensorFlow Version = {}'.format(tf.__version__))\n",
    "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "print('DEVICES', device_lib.list_local_devices())\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Multi-worker configuration\n",
    "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Preparing dataset\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "    # Load train, validation and test sets\n",
    "    dest = 'gs://data-bucket-417812395597/'\n",
    "    train_x = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'train_x', binary_mode=True)\n",
    "    ))\n",
    "    train_y = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'train_y', binary_mode=True)\n",
    "    ))\n",
    "    val_x = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'val_x', binary_mode=True)\n",
    "    ))\n",
    "    val_y = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'val_y', binary_mode=True)\n",
    "    ))\n",
    "    test_x = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'test_x', binary_mode=True)\n",
    "    ))\n",
    "    test_y = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'test_y', binary_mode=True)\n",
    "    ))\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "\n",
    "def tf_model():\n",
    "    black_n_white_input = tensorflow.keras.layers.Input(shape=(80, 80, 1))\n",
    "    \n",
    "    enc = black_n_white_input\n",
    "    \n",
    "    #Encoder part\n",
    "    enc = tensorflow.keras.layers.Conv2D(\n",
    "        32, kernel_size=3, strides=2, padding='same'\n",
    "    )(enc)\n",
    "    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n",
    "    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n",
    "    \n",
    "    enc = tensorflow.keras.layers.Conv2D(\n",
    "        64, kernel_size=3, strides=2, padding='same'\n",
    "    )(enc)\n",
    "    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n",
    "    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n",
    "    \n",
    "    enc = tensorflow.keras.layers.Conv2D(\n",
    "        128, kernel_size=3, strides=2, padding='same'\n",
    "    )(enc)\n",
    "    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n",
    "    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n",
    "    \n",
    "    enc = tensorflow.keras.layers.Conv2D(\n",
    "        256, kernel_size=1, strides=2, padding='same'\n",
    "    )(enc)\n",
    "    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n",
    "    enc = tensorflow.keras.layers.Dropout(0.5)(enc)\n",
    "    \n",
    "    #Decoder part\n",
    "    dec = enc\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2DTranspose(\n",
    "        256, kernel_size=3, strides=2, padding='same'\n",
    "    )(dec)\n",
    "    dec = tensorflow.keras.layers.Activation('relu')(dec)\n",
    "    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2DTranspose(\n",
    "        128, kernel_size=3, strides=2, padding='same'\n",
    "    )(dec)\n",
    "    dec = tensorflow.keras.layers.Activation('relu')(dec)\n",
    "    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2DTranspose(\n",
    "        64, kernel_size=3, strides=2, padding='same'\n",
    "    )(dec)\n",
    "    dec = tensorflow.keras.layers.Activation('relu')(dec)\n",
    "    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2DTranspose(\n",
    "        32, kernel_size=3, strides=2, padding='same'\n",
    "    )(dec)\n",
    "    dec = tensorflow.keras.layers.Activation('relu')(dec)\n",
    "    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2D(\n",
    "        3, kernel_size=3, padding='same'\n",
    "    )(dec)\n",
    "    \n",
    "    color_image = tensorflow.keras.layers.Activation('tanh')(dec)\n",
    "    \n",
    "    return black_n_white_input, color_image\n",
    "\n",
    "# Build the and compile TF model\n",
    "def build_and_compile_tf_model():\n",
    "    black_n_white_input, color_image = tf_model()\n",
    "    model = tensorflow.keras.models.Model(\n",
    "        inputs=black_n_white_input,\n",
    "        outputs=color_image\n",
    "    )\n",
    "    _optimizer = tensorflow.keras.optimizers.Adam(\n",
    "        learning_rate=0.0002,\n",
    "        beta_1=0.5\n",
    "    )\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=_optimizer\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n",
    "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
    "\n",
    "train_x, train_y, _, _, _, _ = make_datasets_unbatched()\n",
    "\n",
    "with strategy.scope():\n",
    "    # Creation of dataset, and model building/compiling need to be within\n",
    "    # `strategy.scope()`.\n",
    "    model = build_and_compile_tf_model()\n",
    "    \n",
    "### Create a TensorBoard call back and write to the gcs path provided by AIP_TENSORBOARD_LOG_DIR\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(\n",
    "  log_dir=os.environ['AIP_TENSORBOARD_LOG_DIR'],\n",
    "  histogram_freq=1)\n",
    "\n",
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=args.epochs,\n",
    "    steps_per_epoch=args.steps,\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b31778-5f81-4198-b5f7-aafea10d059d",
   "metadata": {},
   "source": [
    "## Setting up service-account (required for TensorBoard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "059c226b-61f9-4936-9490-77250d843d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Service Account: 417812395597-compute@developer.gserviceaccount.com\n"
     ]
    }
   ],
   "source": [
    "SERVICE_ACCOUNT=\"dummy-sa\"\n",
    "IS_COLAB=False\n",
    "if (\n",
    "    SERVICE_ACCOUNT == \"\"\n",
    "    or SERVICE_ACCOUNT is None\n",
    "    or SERVICE_ACCOUNT == \"dummy-sa\"\n",
    "):\n",
    "    # Get your service account from gcloud\n",
    "    if not IS_COLAB:\n",
    "        shell_output = ! gcloud auth list 2>/dev/null\n",
    "        SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "\n",
    "    else:  # IS_COLAB:\n",
    "        shell_output = ! gcloud projects describe  $PROJECT_ID\n",
    "        project_number = shell_output[-1].split(\":\")[1].strip().replace(\"'\", \"\")\n",
    "        SERVICE_ACCOUNT = f\"{project_number}-compute@developer.gserviceaccount.com\"\n",
    "\n",
    "    print(\"Service Account:\", SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bccaeaf4-d2bc-4b7d-b432-a3fc0aaa1c72",
   "metadata": {},
   "source": [
    "## Create TensorBoard Instance for monitoring the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fa3c6ae-1c42-4ed6-9266-12bb37e2497d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Tensorboard\n",
      "Create Tensorboard backing LRO: projects/417812395597/locations/us-west2/tensorboards/3940649673949184000/operations/683687325267394560\n",
      "Tensorboard created. Resource name: projects/417812395597/locations/us-west2/tensorboards/3940649673949184000\n",
      "To use this Tensorboard in another session:\n",
      "tb = aiplatform.Tensorboard('projects/417812395597/locations/us-west2/tensorboards/3940649673949184000')\n",
      "TensorBoard resource name: projects/417812395597/locations/us-west2/tensorboards/3940649673949184000\n"
     ]
    }
   ],
   "source": [
    "TENSORBOARD_NAME = \"training-monitoring\"  # @param {type:\"string\"}\n",
    "\n",
    "if (\n",
    "    TENSORBOARD_NAME == \"\"\n",
    "    or TENSORBOARD_NAME is None\n",
    "    or TENSORBOARD_NAME == \"training-monitoring\"\n",
    "):\n",
    "    TENSORBOARD_NAME = PROJECT_ID + \"-tb-\" + TIMESTAMP\n",
    "\n",
    "tensorboard = aiplatform.Tensorboard.create(\n",
    "    display_name=TENSORBOARD_NAME, project=PROJECT_ID, location=REGION\n",
    ")\n",
    "TENSORBOARD_RESOURCE_NAME = tensorboard.gca_resource.name\n",
    "print(\"TensorBoard resource name:\", TENSORBOARD_RESOURCE_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7770fc8-9e79-4220-b40f-5763e3237ab1",
   "metadata": {},
   "source": [
    "## Create staging bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "214bcb28-6e84-46ec-a3fd-5876f3e3614a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://tensorboard-staging/...\n",
      "ServiceException: 409 A Cloud Storage bucket named 'tensorboard-staging' already exists. Try another name. Bucket names must be globally unique across all Google Cloud projects, including those outside of your organization.\n"
     ]
    }
   ],
   "source": [
    "BUCKET_URI = \"gs://tensorboard-staging\"  # @param {type:\"string\"}\n",
    "\n",
    "if BUCKET_URI == \"\" or BUCKET_URI is None or BUCKET_URI == \"gs://[your-bucket-name]\":\n",
    "    BUCKET_URI = \"gs://\" + PROJECT_ID + \"aip-\" + TIMESTAMP\n",
    "\n",
    "! gsutil mb -l {REGION} -p {PROJECT_ID} {BUCKET_URI}\n",
    "    \n",
    "GCS_BUCKET_OUTPUT = BUCKET_URI + \"/output/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c26e9-44d5-498a-a260-5b7f12e4d0ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define and submit vertex AI training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15d811df-4ab0-4231-a2af-92a01d292f94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://tensorboard-staging/output/tensorboard-example-job-20230419074529/aiplatform-2023-04-19-07:45:51.422-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://tensorboard-staging/output/tensorboard-example-job-20230419074529/aiplatform-custom-training-2023-04-19-07:45:51.531 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west2/training/6354706617568591872?project=417812395597\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west2/training/2179377181786898432?project=417812395597\n",
      "View tensorboard:\n",
      "https://us-west2.tensorboard.googleusercontent.com/experiment/projects+417812395597+locations+us-west2+tensorboards+3940649673949184000+experiments+2179377181786898432\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/417812395597/locations/us-west2/trainingPipelines/6354706617568591872\n",
      "Model available at projects/417812395597/locations/us-west2/models/7365091832797069312\n"
     ]
    }
   ],
   "source": [
    "JOB_NAME = \"tensorboard-example-job-{}\".format(TIMESTAMP)\n",
    "BASE_OUTPUT_DIR = \"{}{}\".format(GCS_BUCKET_OUTPUT, JOB_NAME)\n",
    "\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task2.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    staging_bucket=BASE_OUTPUT_DIR,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"tf_bnw_to_color_tb\"\n",
    "# Start the training job\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    service_account=SERVICE_ACCOUNT,\n",
    "    tensorboard=TENSORBOARD_RESOURCE_NAME,\n",
    "    args=CMDARGS,\n",
    "    machine_type = \"n1-standard-8\",\n",
    "    replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b17b84b-1e3d-43d6-ab02-c9f4342e6313",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "37e0a722-dcd8-4c7b-9027-5cd9dab39f1a",
   "metadata": {},
   "source": [
    "## Batch Prediction on Trained Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ff1546-71ac-4754-b520-eb4bccafc681",
   "metadata": {},
   "source": [
    "### load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a143a42-9ef4-4575-b261-ddb4029a00e7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-19 08:24:22.855636: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-19 08:24:24.811215: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-19 08:24:24.811629: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64\n",
      "2023-04-19 08:24:24.811653: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from tensorflow.python.lib.io import file_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "430ffc00-1b34-4ed5-98a7-5c15c5e5b5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1476, 80, 80, 1) (1476, 80, 80, 3)\n"
     ]
    }
   ],
   "source": [
    "dest = 'gs://data-bucket-417812395597/'\n",
    "test_x = np.load(BytesIO(file_io.read_file_to_string(dest+'test_x', binary_mode=True)))\n",
    "test_y = np.load(BytesIO(file_io.read_file_to_string(dest+'test_y', binary_mode=True)))\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03184508-f82c-4f61-b5db-380d0dde11e8",
   "metadata": {},
   "source": [
    "### Creating json payload of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e04b9e8c-5ab0-4ec7-b362-506c930aa2b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://batch_prediction_instances.jsonl...\n",
      "/ [1 files][205.5 MiB/205.5 MiB]                                                \n",
      "Operation completed over 1 objects/205.5 MiB.                                    \n",
      "Uploaded instances to:  gs://tensorboard-staging/batch_prediction_instances/batch_prediction_instances.jsonl\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "BATCH_PREDICTION_INSTANCES_FILE = \"batch_prediction_instances.jsonl\"\n",
    "\n",
    "BATCH_PREDICTION_GCS_SOURCE = (\n",
    "    BUCKET_URI + \"/batch_prediction_instances/\" + BATCH_PREDICTION_INSTANCES_FILE\n",
    ")\n",
    "\n",
    "# converting to searializable format\n",
    "x_test = [(image).astype(np.float32).tolist() for image in test_x]\n",
    "\n",
    "# Write instances at JSONL\n",
    "with open(BATCH_PREDICTION_INSTANCES_FILE, \"w\") as f:\n",
    "    for x in x_test:\n",
    "        f.write(json.dumps(x) + \"\\n\")\n",
    "\n",
    "# Upload to Cloud Storage bucket\n",
    "! gsutil cp batch_prediction_instances.jsonl BATCH_PREDICTION_GCS_SOURCE\n",
    "\n",
    "print(\"Uploaded instances to: \", BATCH_PREDICTION_GCS_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cd2a3f-d60e-48ec-be00-7377d9ae04f4",
   "metadata": {},
   "source": [
    "### make batch prediction request "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52da32b6-0fc6-46b6-a3d2-daf575314cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating BatchPredictionJob\n",
      "BatchPredictionJob created. Resource name: projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864\n",
      "To use this BatchPredictionJob in another session:\n",
      "bpj = aiplatform.BatchPredictionJob('projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864')\n",
      "View Batch Prediction Job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west2/batch-predictions/4956338933270052864?project=417812395597\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_RUNNING\n",
      "BatchPredictionJob projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864 current state:\n",
      "JobState.JOB_STATE_SUCCEEDED\n",
      "BatchPredictionJob run completed. Resource name: projects/417812395597/locations/us-west2/batchPredictionJobs/4956338933270052864\n"
     ]
    }
   ],
   "source": [
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "# The name of the job\n",
    "BATCH_PREDICTION_JOB_NAME = \"bnw_to_color_batch_prediction\"\n",
    "\n",
    "# Folder in the bucket to write results to\n",
    "DESTINATION_FOLDER = \"batch_prediction_results\"\n",
    "\n",
    "# The Cloud Storage bucket to upload results to\n",
    "BATCH_PREDICTION_GCS_DEST_PREFIX = BUCKET_URI + \"/\" + DESTINATION_FOLDER\n",
    "\n",
    "# Make SDK batch_predict method call\n",
    "batch_prediction_job = model.batch_predict(\n",
    "    instances_format=\"jsonl\",\n",
    "    predictions_format=\"jsonl\",\n",
    "    job_display_name=BATCH_PREDICTION_JOB_NAME,\n",
    "    gcs_source=BATCH_PREDICTION_GCS_SOURCE,\n",
    "    gcs_destination_prefix=BATCH_PREDICTION_GCS_DEST_PREFIX,\n",
    "    model_parameters=None,\n",
    "    starting_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    "    machine_type=\"n1-standard-4\",\n",
    "    sync=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44c735-10f4-4eac-99c4-42edb12e3bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
