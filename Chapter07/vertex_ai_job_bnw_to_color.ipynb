{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b80af36d-be92-44bb-b84f-e7de7d7a038c",
   "metadata": {},
   "source": [
    "# Convert Black-n-White image to Color Images - Vertex AI Training Job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3568b64-9299-4eb8-a99d-3b7c6ddda61b",
   "metadata": {},
   "source": [
    "## Install useful packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23baac0a-f082-43f3-a564-20d37bb6694f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-cloud-aiplatform in /opt/conda/lib/python3.7/site-packages (1.21.0)\n",
      "Collecting google-cloud-aiplatform\n",
      "  Downloading google_cloud_aiplatform-1.24.0-py2.py3-none-any.whl (2.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-storage in /opt/conda/lib/python3.7/site-packages (2.7.0)\n",
      "Collecting google-cloud-storage\n",
      "  Downloading google_cloud_storage-2.8.0-py2.py3-none-any.whl (113 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m113.6/113.6 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pillow in /opt/conda/lib/python3.7/site-packages (9.4.0)\n",
      "Collecting pillow\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-manylinux_2_28_x86_64.whl (3.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m79.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5\n",
      "  Downloading protobuf-4.22.3-cp37-abi3-manylinux2014_x86_64.whl (302 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.4/302.4 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting packaging<22.0.0dev,>=14.3\n",
      "  Downloading packaging-21.3-py3-none-any.whl (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.8/40.8 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-cloud-bigquery<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (3.4.2)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.8.1)\n",
      "Requirement already satisfied: shapely<2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.8.5.post1)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (1.22.2)\n",
      "Requirement already satisfied: google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-aiplatform) (2.10.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.3.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.4.1)\n",
      "Requirement already satisfied: google-auth<3.0dev,>=1.25.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.16.0)\n",
      "Requirement already satisfied: requests<3.0.0dev,>=2.18.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.28.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.7/site-packages (from google-cloud-storage) (2.3.2)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.56.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.58.0)\n",
      "Requirement already satisfied: grpcio-status<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.48.2)\n",
      "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /opt/conda/lib/python3.7/site-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0dev,>=1.32.0->google-cloud-aiplatform) (1.51.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (5.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (4.9)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.2.8)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/conda/lib/python3.7/site-packages (from google-auth<3.0dev,>=1.25.0->google-cloud-storage) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil<3.0dev,>=2.7.2 in /opt/conda/lib/python3.7/site-packages (from google-cloud-bigquery<4.0.0dev,>=1.15.0->google-cloud-aiplatform) (2.8.2)\n",
      "Requirement already satisfied: grpc-google-iam-v1<1.0.0dev,>=0.12.4 in /opt/conda/lib/python3.7/site-packages (from google-cloud-resource-manager<3.0.0dev,>=1.3.3->google-cloud-aiplatform) (0.12.6)\n",
      "Collecting google-cloud-resource-manager<3.0.0dev,>=1.3.3\n",
      "  Downloading google_cloud_resource_manager-1.9.1-py2.py3-none-any.whl (276 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.9.0-py2.py3-none-any.whl (276 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m276.8/276.8 kB\u001b[0m \u001b[31m31.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.8.0-py2.py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.7.0-py2.py3-none-any.whl (235 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.3/235.3 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Downloading google_cloud_resource_manager-1.6.3-py2.py3-none-any.whl (233 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.8/233.8 kB\u001b[0m \u001b[31m29.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: google-crc32c<2.0dev,>=1.0 in /opt/conda/lib/python3.7/site-packages (from google-resumable-media>=2.3.2->google-cloud-storage) (1.5.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging<22.0.0dev,>=14.3->google-cloud-aiplatform) (3.0.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.7/site-packages (from requests<3.0.0dev,>=2.18.0->google-cloud-storage) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /opt/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0dev,>=1.25.0->google-cloud-storage) (0.4.8)\n",
      "Installing collected packages: protobuf, pillow, packaging, google-cloud-storage, google-cloud-resource-manager, google-cloud-aiplatform\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.1\n",
      "    Uninstalling protobuf-3.20.1:\n",
      "      Successfully uninstalled protobuf-3.20.1\n",
      "  Attempting uninstall: pillow\n",
      "    Found existing installation: Pillow 9.4.0\n",
      "    Uninstalling Pillow-9.4.0:\n",
      "      Successfully uninstalled Pillow-9.4.0\n",
      "  Attempting uninstall: packaging\n",
      "    Found existing installation: packaging 23.0\n",
      "    Uninstalling packaging-23.0:\n",
      "      Successfully uninstalled packaging-23.0\n",
      "  Attempting uninstall: google-cloud-storage\n",
      "    Found existing installation: google-cloud-storage 2.7.0\n",
      "    Uninstalling google-cloud-storage-2.7.0:\n",
      "      Successfully uninstalled google-cloud-storage-2.7.0\n",
      "  Attempting uninstall: google-cloud-resource-manager\n",
      "    Found existing installation: google-cloud-resource-manager 1.8.1\n",
      "    Uninstalling google-cloud-resource-manager-1.8.1:\n",
      "      Successfully uninstalled google-cloud-resource-manager-1.8.1\n",
      "  Attempting uninstall: google-cloud-aiplatform\n",
      "    Found existing installation: google-cloud-aiplatform 1.21.0\n",
      "    Uninstalling google-cloud-aiplatform-1.21.0:\n",
      "      Successfully uninstalled google-cloud-aiplatform-1.21.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.8.4 requires protobuf<3.20,>=3.9.2, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorflow-transform 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorflow-metadata 0.14.0 requires protobuf<4,>=3.7, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorboardx 2.5.1 requires protobuf<=3.20.1,>=3.8.0, but you have protobuf 4.22.3 which is incompatible.\n",
      "tensorboard 2.8.0 requires google-auth-oauthlib<0.5,>=0.4.1, but you have google-auth-oauthlib 0.8.0 which is incompatible.\n",
      "google-cloud-vision 3.3.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-videointelligence 1.16.3 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-spanner 3.27.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-pubsub 2.14.0 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-language 1.3.2 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-dlp 3.11.1 requires google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.10.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,!=2.8.*,!=2.9.*,<3.0.0dev,>=1.34.0, but you have google-api-core 2.10.1 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-bigtable 1.7.3 requires protobuf<4.0.0dev, but you have protobuf 4.22.3 which is incompatible.\n",
      "google-cloud-bigquery-storage 2.13.2 requires protobuf<4.0.0dev,>=3.19.0, but you have protobuf 4.22.3 which is incompatible.\n",
      "apache-beam 2.44.0 requires httplib2<0.21.0,>=0.8, but you have httplib2 0.21.0 which is incompatible.\n",
      "apache-beam 2.44.0 requires protobuf<4,>3.12.2, but you have protobuf 4.22.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-cloud-aiplatform-1.24.0 google-cloud-resource-manager-1.6.3 google-cloud-storage-2.8.0 packaging-21.3 pillow-9.5.0 protobuf-4.22.3\n"
     ]
    }
   ],
   "source": [
    "# Install the packages\n",
    "! pip3 install --upgrade google-cloud-aiplatform \\\n",
    "                        google-cloud-storage \\\n",
    "                        pillow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd52bf-29ab-4851-9b21-ff0ad7814b84",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7e93550-5b0d-4558-9cd8-6c5fb7465f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from google.cloud import aiplatform\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61787ad8-d88a-4129-b521-7456074c6a49",
   "metadata": {},
   "source": [
    "## Setup project configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c80379c-99e9-495e-89bf-d0fea02db474",
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID='417812395597'\n",
    "REGION='us-west2'\n",
    "BUCKET_URI='gs://my-training-artifacts'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2657b6b8-17b0-4e7a-a454-d31fe3018161",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI (SDK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0108c61b-7a06-48c8-bffd-7da85f2bbf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=REGION, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa928772-f392-4917-b0c0-1ab5c6bff483",
   "metadata": {},
   "source": [
    "## Setup pre-built contianer for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c10544f7-f248-4221-bcc0-d324aa1e6cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_VERSION = \"tf-cpu.2-9\"\n",
    "DEPLOY_VERSION = \"tf2-cpu.2-9\"\n",
    "TRAIN_IMAGE = \"us-docker.pkg.dev/vertex-ai/training/{}:latest\".format(TRAIN_VERSION)\n",
    "DEPLOY_IMAGE = \"us-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(DEPLOY_VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ca7270-4c5b-4900-9dee-630ce55942c0",
   "metadata": {},
   "source": [
    "## Define command-line arguments for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45e7bcda-457a-4802-a3d9-325d3a6c9e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "JOB_NAME = \"vertex_custom_training\"\n",
    "MODEL_DIR = \"{}/{}\".format(BUCKET_URI, JOB_NAME)\n",
    "\n",
    "TRAIN_STRATEGY = \"single\"\n",
    "EPOCHS = 20\n",
    "STEPS = 100\n",
    "\n",
    "CMDARGS = [\n",
    "    \"--epochs=\" + str(EPOCHS),\n",
    "    \"--steps=\" + str(STEPS),\n",
    "    \"--distribute=\" + TRAIN_STRATEGY,\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e0dbed-e186-4361-bcb1-67feab3ed238",
   "metadata": {},
   "source": [
    "## Writing down entire training script into a \"task.py\" file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f2748a-4e7c-40d2-903d-63f7a8121e2e",
   "metadata": {},
   "source": [
    "### This task.py will run inside the contianer that we have defined above (could be a custom contianer with all dependencies)\n",
    "#### task.py should have the entire training flow including - \n",
    "- Load and prepared the training data\n",
    "- define model architecure\n",
    "- train model\n",
    "- save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ccb42302-2dc4-4f31-9489-3dcddde878ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile task.py\n",
    "# Single, Mirror and Multi-Machine Distributed Training\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow\n",
    "from tensorflow.python.client import device_lib\n",
    "import argparse\n",
    "import os\n",
    "import sys\n",
    "from io import BytesIO\n",
    "import numpy as np\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "# parse required arguments\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--lr', dest='lr',\n",
    "                    default=0.001, type=float,\n",
    "                    help='Learning rate.')\n",
    "parser.add_argument('--epochs', dest='epochs',\n",
    "                    default=10, type=int,\n",
    "                    help='Number of epochs.')\n",
    "parser.add_argument('--steps', dest='steps',\n",
    "                    default=35, type=int,\n",
    "                    help='Number of steps per epoch.')\n",
    "parser.add_argument('--distribute', dest='distribute', type=str, default='single',\n",
    "                    help='distributed training strategy')\n",
    "args = parser.parse_args()\n",
    "\n",
    "print('Python Version = {}'.format(sys.version))\n",
    "print('TensorFlow Version = {}'.format(tf.__version__))\n",
    "print('TF_CONFIG = {}'.format(os.environ.get('TF_CONFIG', 'Not found')))\n",
    "print('DEVICES', device_lib.list_local_devices())\n",
    "\n",
    "# Single Machine, single compute device\n",
    "if args.distribute == 'single':\n",
    "    if tf.test.is_gpu_available():\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/gpu:0\")\n",
    "    else:\n",
    "        strategy = tf.distribute.OneDeviceStrategy(device=\"/cpu:0\")\n",
    "# Single Machine, multiple compute device\n",
    "elif args.distribute == 'mirror':\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "# Multiple Machine, multiple compute device\n",
    "elif args.distribute == 'multi':\n",
    "    strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Multi-worker configuration\n",
    "print('num_replicas_in_sync = {}'.format(strategy.num_replicas_in_sync))\n",
    "\n",
    "# Preparing dataset\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "def make_datasets_unbatched():\n",
    "    # Load train, validation and test sets\n",
    "    dest = 'gs://data-bucket-417812395597/'\n",
    "    train_x = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'train_x', binary_mode=True)\n",
    "    ))\n",
    "    train_y = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'train_y', binary_mode=True)\n",
    "    ))\n",
    "    val_x = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'val_x', binary_mode=True)\n",
    "    ))\n",
    "    val_y = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'val_y', binary_mode=True)\n",
    "    ))\n",
    "    test_x = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'test_x', binary_mode=True)\n",
    "    ))\n",
    "    test_y = np.load(BytesIO(\n",
    "        file_io.read_file_to_string(dest+'test_y', binary_mode=True)\n",
    "    ))\n",
    "    return train_x, train_y, val_x, val_y, test_x, test_y\n",
    "\n",
    "def tf_model():\n",
    "    black_n_white_input = tensorflow.keras.layers.Input(shape=(80, 80, 1))\n",
    "    \n",
    "    enc = black_n_white_input\n",
    "    \n",
    "    #Encoder part\n",
    "    enc = tensorflow.keras.layers.Conv2D(\n",
    "        32, kernel_size=3, strides=2, padding='same'\n",
    "    )(enc)\n",
    "    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n",
    "    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n",
    "    \n",
    "    enc = tensorflow.keras.layers.Conv2D(\n",
    "        64, kernel_size=3, strides=2, padding='same'\n",
    "    )(enc)\n",
    "    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n",
    "    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n",
    "    \n",
    "    enc = tensorflow.keras.layers.Conv2D(\n",
    "        128, kernel_size=3, strides=2, padding='same'\n",
    "    )(enc)\n",
    "    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n",
    "    enc = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(enc)\n",
    "    \n",
    "    enc = tensorflow.keras.layers.Conv2D(\n",
    "        256, kernel_size=1, strides=2, padding='same'\n",
    "    )(enc)\n",
    "    enc = tensorflow.keras.layers.LeakyReLU(alpha=0.2)(enc)\n",
    "    enc = tensorflow.keras.layers.Dropout(0.5)(enc)\n",
    "    \n",
    "    #Decoder part\n",
    "    dec = enc\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2DTranspose(\n",
    "        256, kernel_size=3, strides=2, padding='same'\n",
    "    )(dec)\n",
    "    dec = tensorflow.keras.layers.Activation('relu')(dec)\n",
    "    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2DTranspose(\n",
    "        128, kernel_size=3, strides=2, padding='same'\n",
    "    )(dec)\n",
    "    dec = tensorflow.keras.layers.Activation('relu')(dec)\n",
    "    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2DTranspose(\n",
    "        64, kernel_size=3, strides=2, padding='same'\n",
    "    )(dec)\n",
    "    dec = tensorflow.keras.layers.Activation('relu')(dec)\n",
    "    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2DTranspose(\n",
    "        32, kernel_size=3, strides=2, padding='same'\n",
    "    )(dec)\n",
    "    dec = tensorflow.keras.layers.Activation('relu')(dec)\n",
    "    dec = tensorflow.keras.layers.BatchNormalization(momentum=0.8)(dec)\n",
    "    \n",
    "    dec = tensorflow.keras.layers.Conv2D(\n",
    "        3, kernel_size=3, padding='same'\n",
    "    )(dec)\n",
    "    \n",
    "    color_image = tensorflow.keras.layers.Activation('tanh')(dec)\n",
    "    \n",
    "    return black_n_white_input, color_image\n",
    "\n",
    "# Build the and compile TF model\n",
    "def build_and_compile_tf_model():\n",
    "    black_n_white_input, color_image = tf_model()\n",
    "    model = tensorflow.keras.models.Model(\n",
    "        inputs=black_n_white_input,\n",
    "        outputs=color_image\n",
    "    )\n",
    "    _optimizer = tensorflow.keras.optimizers.Adam(\n",
    "        learning_rate=0.0002,\n",
    "        beta_1=0.5\n",
    "    )\n",
    "    model.compile(\n",
    "        loss='mse',\n",
    "        optimizer=_optimizer\n",
    "    )\n",
    "    return model\n",
    "\n",
    "# Train the model\n",
    "NUM_WORKERS = strategy.num_replicas_in_sync\n",
    "# Here the batch size scales up by number of workers since\n",
    "# `tf.data.Dataset.batch` expects the global batch size.\n",
    "GLOBAL_BATCH_SIZE = BATCH_SIZE * NUM_WORKERS\n",
    "MODEL_DIR = os.getenv(\"AIP_MODEL_DIR\")\n",
    "\n",
    "train_x, train_y, _, _, _, _ = make_datasets_unbatched()\n",
    "\n",
    "with strategy.scope():\n",
    "    # Creation of dataset, and model building/compiling need to be within\n",
    "    # `strategy.scope()`.\n",
    "    model = build_and_compile_tf_model()\n",
    "\n",
    "model.fit(\n",
    "    train_x,\n",
    "    train_y,\n",
    "    epochs=args.epochs,\n",
    "    steps_per_epoch=args.steps\n",
    ")\n",
    "model.save(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1c26e9-44d5-498a-a260-5b7f12e4d0ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Define and submit vertex AI training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "651e5fbc-f220-47cc-9095-9e9fc7511368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training script copied to:\n",
      "gs://my-training-artifacts/aiplatform-2023-04-18-09:25:16.215-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "Training Output directory:\n",
      "gs://my-training-artifacts/aiplatform-custom-training-2023-04-18-09:25:16.320 \n",
      "View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west2/training/3454177351309459456?project=417812395597\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/3454177351309459456 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-west2/training/6460048627602554880?project=417812395597\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/3454177351309459456 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/3454177351309459456 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/3454177351309459456 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/3454177351309459456 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/3454177351309459456 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob projects/417812395597/locations/us-west2/trainingPipelines/3454177351309459456 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "CustomTrainingJob run completed. Resource name: projects/417812395597/locations/us-west2/trainingPipelines/3454177351309459456\n",
      "Model available at projects/417812395597/locations/us-west2/models/1132109948516302848\n"
     ]
    }
   ],
   "source": [
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name=JOB_NAME,\n",
    "    script_path=\"task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[],\n",
    "    model_serving_container_image_uri=DEPLOY_IMAGE,\n",
    ")\n",
    "\n",
    "MODEL_DISPLAY_NAME = \"tf_bnw_to_color\"\n",
    "\n",
    "# Start the training job\n",
    "model = job.run(\n",
    "    model_display_name=MODEL_DISPLAY_NAME,\n",
    "    args=CMDARGS,\n",
    "    machine_type = \"n1-standard-16\",\n",
    "    replica_count=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7492a1ce-594b-41d4-bedf-a12f3e9c3c79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d44c735-10f4-4eac-99c4-42edb12e3bf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-8.m103",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-8:m103"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
